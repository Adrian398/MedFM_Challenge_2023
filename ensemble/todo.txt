[X] For colon:
    When training new models:
        1 log ACCURACY
        2 save best ACCURACY

[X] For chest / endo:
    When training new models:
        1 log mAP
        2 Save best MAP

In general:
[X] Add copying of used configs to some directory in the submission directory (or add a report summarizing used models)
[X] Local validation:
    - download annotations for validation data
    - make inferences on validation data => 2 csvs per run script
    - make sure infer_missing_predictions is adjusted, so it checks for the correct csv (not just any)
    - adapt ensemble/submission to use inferences on validation data to create ensemble result =>
        - Write custom evaluation of these results (can't use Runner / test script, cause that requires one model,
        we have csvs from multiple models)


[X] 1) Adjust submission.py so that the user can choose between "evaluation" and "validation" submission -> save results in a different folder
[X] 2) Adjust submission.py so that a report is written for all the models used in the ensemble -> output to validation folder
[X] 3) Write a script that takes the submission csvs and compares them with the downloaded labelled validation data to compare scores (per exp, setting and overall)
    - Input is path to the validation submission dir and path to the annotation file
    - We have prediction probabilities and labels -> calculate mAP and AUC (metrics)
[X] 4) In submission.py implement different "ensemble strategies" and assess those with the previously written script


[X] Recreate all Endo Submission and Validation prediction CSVs with pre-processed images
[X] Check and align submission csv files
[X] Softmax probabilities for colon in ensemble/submission -> almost no effect on validation set


[ ] Add Multiprocessing to task_submission to improve performance
[ ] Add "expert-per-setting" ensemble strategy
[ ] Extend ensemble gridsearch to search the best ensemble strategy among "tasks" & "n-shot" instead of only "tasks"